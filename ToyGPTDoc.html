<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="" />
    <link
      rel="stylesheet"
      as="style"
      onload="this.rel='stylesheet'"
      href="https://fonts.googleapis.com/css2?display=swap&amp;family=Noto+Sans%3Awght%40400%3B500%3B700%3B900&amp;family=Space+Grotesk%3Awght%40400%3B500%3B700"
    />
    <title>Training Loss Visualization</title>
    <script src="https://cdn.tailwindcss.com?plugins=forms,container-queries"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="" />
    <link
      rel="stylesheet"
      as="style"
      onload="this.rel='stylesheet'"
      href="https://fonts.googleapis.com/css2?display=swap&amp;family=Noto+Sans%3Awght%40400%3B500%3B700%3B900&amp;family=Space+Grotesk%3Awght%40400%3B500%3B700"
    />
    <title>Building a GPT Model from Scratch | Dr. Evelyn Hayes</title>
    <link rel="icon" type="image/x-icon" href="data:image/x-icon;base64," />
    <script src="https://cdn.tailwindcss.com?plugins=forms,container-queries"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.9.1/chart.min.js"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: '#101923',
                        secondary: '#223549',
                        accent: '#90accb',
                    }
                }
            }
        }
    </script>
    <style>
        .toc-link {
            position: relative;
            padding-left: 1.5rem;
        }
        
        .toc-link:before {
            content: "";
            position: absolute;
            left: 0;
            top: 50%;
            transform: translateY(-50%);
            width: 8px;
            height: 8px;
            background-color: #90accb;
            border-radius: 50%;
        }
        
        .toc-link:hover:before {
            background-color: white;
        }
        
        .code-block {
            background-color: #1a2332;
            border-radius: 0.5rem;
            padding: 1.5rem;
            overflow-x: auto;
        }
        
        .code-block pre {
            margin: 0;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
            font-size: 0.875rem;
            line-height: 1.5;
        }
        
        .code-block code {
            color: #90accb;
        }
        
        .generated-text {
            background-color: #1a2332;
            border-left: 4px solid #90accb;
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
            white-space: pre-wrap;
        }
        
        .section-highlight {
            background-color: rgba(144, 172, 203, 0.1);
            border-left: 4px solid #90accb;
            padding: 0.5rem 1rem;
            margin: 1rem 0;
        }

        .description {
            background-color: #031830;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 15px;
            border-left: 4px solid #4CAF50;
        }
        
        .chart-container {
            background-color: #1a2332;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }
        h1 {
            color: white;
            text-align: center;
            margin-bottom: 30px;
        }
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }
        .metric-card {
            background-color: #223549;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }
        .metric-value {
            font-size: 2em;
            font-weight: bold;
            color: #90accb;
        }
        .metric-label {
            font-size: 0.9em;
            color: #90accb;
            opacity: 0.8;
        }
        
    </style>
</head>
<body class="bg-primary text-accent font-['Space_Grotesk','Noto_Sans',sans-serif]">
    <!-- HEADER -->
    <header class="sticky top-0 z-50 flex items-center justify-between border-b border-[#223549] bg-[#0d1929] px-10 py-3">



          <!-- <div class="flex items-center gap-4 text-white">
            <div class="size-4">
              <svg viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path
                  fill-rule="evenodd"
                  clip-rule="evenodd"
                  d="M24 18.4228L42 11.475V34.3663C42 34.7796 41.7457 35.1504 41.3601 35.2992L24 42V18.4228Z"
                  fill="currentColor"
                ></path>
                <path
                  fill-rule="evenodd"
                  clip-rule="evenodd"
                  d="M24 8.18819L33.4123 11.574L24 15.2071L14.5877 11.574L24 8.18819ZM9 15.8487L21 20.4805V37.6263L9 32.9945V15.8487ZM27 37.6263V20.4805L39 15.8487V32.9945L27 37.6263ZM25.354 2.29885C24.4788 1.98402 23.5212 1.98402 22.646 2.29885L4.98454 8.65208C3.7939 9.08038 3 10.2097 3 11.475V34.3663C3 36.0196 4.01719 37.5026 5.55962 38.098L22.9197 44.7987C23.6149 45.0671 24.3851 45.0671 25.0803 44.7987L42.4404 38.098C43.9828 37.5026 45 36.0196 45 34.3663V11.475C45 10.2097 44.2061 9.08038 43.0155 8.65208L25.354 2.29885Z"
                  fill="currentColor"
                ></path>
              </svg>
            </div>
            <h2 class="text-white text-lg font-bold leading-tight tracking-[-0.015em]">Dr. Evelyn Hayes</h2>
          </div> -->
          <div class="flex flex-1 justify-center gap-8">
            <div class="flex items-center gap-9">
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/"><strong>HOME</strong></a>
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/#work">Work</a>
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/#education">Education</a>
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/#journey">Journey</a>
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/#technical">Technical Arsenal</a>
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/#cartifications">Certifications</a>
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/#publications">Publications</a>
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/#achievements">Achievements</a>
              <a class="text-white text-sm font-medium leading-normal" href="/MUHAMMAD-AHMAD/#contact">Contact</a>
            </div>
          </div>
        </header>
    
    <!-- MAIN CONTENT -->
    <main class="flex flex-1 justify-center py-8 px-4 md:mx-32">
        <div class="w-full max-w-4xl">
            
            <!-- ARTICLE HEADER -->
            <article class="prose prose-invert max-w-none">
                <div class="text-center mb-12">
                    <span class="inline-block px-4 py-1 bg-secondary rounded-full text-accent text-sm mb-6">
                        Data Science and AI
                    </span>
                    
                    <h1 class="text-white text-3xl md:text-5xl font-black leading-tight tracking-[-0.033em] mb-6">
                        Building a GPT Model from Scratch
                    </h1>
                    
                    <p class="text-accent text-base md:text-lg max-w-3xl mx-auto leading-relaxed mb-8">
                        A step-by-step guide to implementing a Generative Pre-trained Transformer from scratch, including data preparation, model architecture, and training.
                    </p>
                    
                    <div class="flex flex-wrap items-center justify-center gap-4 md:gap-8 text-accent text-sm">
                        <div class="flex items-center gap-2">
                            <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 20 20">
                                <path fill-rule="evenodd" d="M6 2a1 1 0 00-1 1v1H4a2 2 0 00-2 2v10a2 2 0 002 2h12a2 2 0 002-2V6a2 2 0 00-2-2h-1V3a1 1 0 10-2 0v1H7V3a1 1 0 00-1-1zm0 5a1 1 0 000 2h8a1 1 0 100-2H6z" clip-rule="evenodd" />
                            </svg>
                            <span>July 16, 2025</span>
                        </div>
                        <div class="flex items-center gap-2">
                            <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 20 20">
                                <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm1-12a1 1 0 10-2 0v4a1 1 0 00.293.707l2.828 2.829a1 1 0 101.415-1.415L11 9.586V6z" clip-rule="evenodd" />
                            </svg>
                            <span>30 min read</span>
                        </div>
                        <span>MUHAMMAD AHMAD</span>
                    </div>
                </div>
                    
                    <!-- Introduction -->
                    <div id="introduction">
                        <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-secondary pb-2">
                            Introduction
                        </h2>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Overview of GPT and its Significance
                        </h3>
                        <p>
                            Generative Pre-trained Transformer (GPT) models have revolutionized natural language processing by enabling machines to generate human-like text. Introduced in the seminal paper by Radford et al. (2018), GPT leverages the transformer architecture, which relies heavily on the attention mechanism to model long-range dependencies in text. This project aims to demystify the GPT architecture by building a simplified version from scratch, focusing on its core components and training process.
                        </p>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Objectives of Building a GPT Model from Scratch
                        </h3>
                        <p>
                            The primary objective of this project was to gain a deep understanding of the transformer architecture by implementing a GPT model from the ground up. By constructing each component data preprocessing, attention mechanisms, feed-forward networks, and training loops I aimed to explore the intricacies of language modeling and demonstrate the ability to generate Shakespearean-style text using the TinyShakespeare dataset.
                        </p>
                        
                    </div>
                    
                    <!-- Data Preparation -->
                    <div id="data-preparation">
                        <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-secondary pb-2">
                            Data Preparation
                        </h2>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Dataset Selection
                        </h3>
                        <p>
                            The TinyShakespeare dataset was chosen for its compact size and rich linguistic structure, making it ideal for training a small-scale language model. This dataset contains a subset of Shakespeare's plays and sonnets, providing a challenging yet manageable corpus for text generation.
                        </p>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Using the TinyShakespeare Dataset
                        </h3>
                        <p>
                            The dataset, accessed on July 16, 2025, from the URL <a href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyShakespeare/Input.txt" class="text-white hover:text-accent">https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyShakespeare/Input.txt</a>, contains approximately 1 million characters of Shakespearean text. Its relatively small size allows for efficient experimentation while capturing the complexity of early modern English.
                        </p>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Accessing the Dataset
                        </h3>
                        <p>
                            The dataset was downloaded and loaded into a Jupyter notebook environment for preprocessing. The URL provided a plain text file, which was read into memory for further processing.
                        </p>

                    <div class="code-block bg-code p-4 rounded-lg my-4">
                                    <pre class="text-sm overflow-x-auto">!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt</pre>
                    </div>

                    <div id="exploring" class="mt-8">
                            <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                                Exploring the Data
                            </h3>
                            <p>
                                Initial exploration reveals the dataset's structure:
                            </p>
                            <div class="code-block bg-code p-4 rounded-lg my-4">
                                <pre class="text-sm overflow-x-auto">with open('input.txt', 'r', encoding='utf-8') as f:
text = f.read()
print(f"{len(text)/1000000} M Characters.")
print(text[:1000])</pre>
                            </div>
                            <div class="bg-highlight p-4 rounded-lg my-4 text-sm italic">
                                <p>Output:</p>
                                <p>1.115394 M Characters.</p>
                                <p>First Citizen:</p>
                                <p>Before we proceed any further, hear me speak.</p>
                                <p>All: Speak, speak.</p>
                                <p>First Citizen: You are all resolved rather to die than to famish?</p>
                            </div>
                        </div>

                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Preprocessing
                        </h3>
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Extracting Unique Characters</h4>
                        <p>
                            The first step in preprocessing was to extract unique characters from the dataset to define the vocabulary. The code snippet below illustrates this process:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>chars = sorted(list(set(text)))
vocab_size = len(chars)</code></pre>
                        </div>
                        
                        <p>
                            This code identifies all unique characters in the text, resulting in a vocabulary size of 65 characters, including letters, punctuation, and special characters like spaces and newlines.
                        </p>
                        
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Defining Vocabulary Size</h4>
                        <p>
                            The vocabulary size (<code>vocab_size = 65</code>) determines the dimensionality of the model's output layer, as it predicts the next character from this set. A larger vocabulary would increase computational complexity, but the TinyShakespeare dataset's size is well-suited for this project.
                        </p>
                        
                        <div class="bg-[#0f172a] p-4 rounded my-4 text-sm">
                            Sr?qP-QWktXoL&amp;jLDJgOLVz'RIoDqHdhsV&amp;vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3
                        </div>

                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Tokenization and Encoding Strategies</h4>
                        <p>
                            Tokenization involved mapping each character to a unique integer index. This character-level encoding simplifies the model compared to word-level tokenization, as it avoids the need for a large vocabulary. The encoding strategy ensures that each character in the input text is represented as an integer, which is then converted into an embedding vector during model training.
                        </p>
                        
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mt-4">
                                    <div class="code-block bg-code p-4 rounded-lg">
                                        <pre class="text-sm overflow-x-auto">string_to_int = {ch:i for i,ch in enumerate(chars)}
encode = lambda s: [string_to_int[c] for c in s]
encode("hello")  # [46, 43, 53, 53, 56]</pre>
                                    </div>
                                    <div class="code-block bg-code p-4 rounded-lg">
                    <pre class="text-sm overflow-x-auto">int_to_string = {i:ch for i,ch in enumerate(chars)}
decode = lambda l: ''.join([int_to_string[i] for i in l])
decode([46,43,53,53,56])  # "hello"</pre>
                                    </div>
                                </div>

                    </div>
                    
                    <!-- Model Architecture -->
                    <div id="model-architecture">
                        <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-secondary pb-2">
                            Model Architecture
                        </h2>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Bigram Model Baseline
                        </h3>
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Introduction to the Simple Bigram Model</h4>
                        <p>
                            As a starting point, I implemented a simple bigram model, which predicts the next character based solely on the current character. This serves as a baseline to understand the limitations of naive language modeling before introducing the transformer architecture.
                        </p>
                        
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Implementation Using PyTorch</h4>
                        <p>
                            The bigram model was implemented using PyTorch, with the following setup:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1377)

class BigramModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, vocab_size)
    
    def forward(self, idx):
        logits = self.embedding(idx)
        return logits</code></pre>
                        </div>
                        
                        <p>
                            This model uses an embedding layer to map input character indices to logits, which represent the probability distribution over the vocabulary for the next character.
                        </p>
                        
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Expected Loss for Random Predictions</h4>
                        <p>
                            For a vocabulary size of 65, the expected loss for random predictions is calculated as the negative log-likelihood of a uniform distribution:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>expected_loss = -torch.log(torch.tensor(1/65))
# Output: ~4.17</code></pre>
                        </div>
                        
                        <p>
                            This loss serves as a benchmark, indicating that a randomly guessing model would achieve a loss of approximately 4.17.
                        </p>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Attention Mechanism
                        </h3>
                        <p>
                            The attention mechanism is the cornerstone of the GPT architecture, enabling the model to weigh the importance of different input tokens when making predictions.
                        </p>
                        
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Single-Head Attention</h4>
                        <div class="section-highlight">
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Computing Query, Key, and Value Vectors</h5>
                            <p>
                                The attention mechanism begins by computing Query (Q), Key (K), and Value (V) vectors for each input token. These vectors are obtained by projecting the input embeddings through linear layers:
                            </p>
                            
                            <div class="code-block my-6">
                                <pre><code>head_size = 16
query = nn.Linear(C, head_size, bias=False)
key = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)</code></pre>
                            </div>
                            
                            <p>
                                Here, <code>C</code> is the embedding dimension, and <code>head_size</code> is set to 16. The resulting tensor shapes are <code>torch.Size([4, 8, 16])</code> for a batch size of 4 and sequence length of 8.
                            </p>
                            
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Attention Score Calculation</h5>
                            <p>
                                Attention scores are computed as the dot product of Query and Key vectors:
                            </p>
                            
                            <div class="code-block my-6">
                                <pre><code>score = query @ key.transpose(-2, -1)</code></pre>
                            </div>
                            
                            <p>
                                This produces a score matrix of shape <code>torch.Size([4, 8, 8])</code>, representing the similarity between each pair of tokens in the sequence.
                            </p>
                            
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Scaling Attention Scores</h5>
                            <p>
                                To prevent large values from dominating the softmax operation, the scores are scaled by the square root of the key dimension:
                            </p>
                            
                            <div class="code-block my-6">
                                <pre><code>k = head_size
scaled_score = score / (k ** 0.5)</code></pre>
                            </div>
                            
                            <p>
                                This scaling stabilizes gradients during training, as shown in the output:
                            </p>
                            
                            <div class="code-block my-6">
                                <pre><code>scaled_score[1]
# tensor([[ 7.2954e-02,  3.9236e-08,  7.5671e-02, -9.2754e-02, -2.2411e-01,
#          6.7716e-01,  5.1685e-01, -2.2411e-01], ...])</code></pre>
                            </div>
                            
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Applying Triangular Masking for Autoregressive Modeling</h5>
                            <p>
                                To ensure the model only attends to previous tokens (autoregressive property), a triangular mask is applied:
                            </p>
                            
                            <div class="code-block my-6">
                                <pre><code>tril = torch.tril(torch.ones(T, T))
masked_scaled_score = scaled_score.masked_fill(tril == 0, float('-inf'))</code></pre>
                            </div>
                            
                            <p>
                                This sets future token scores to negative infinity, ensuring they have zero probability after the softmax operation.
                            </p>
                            
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Softmax Application for Attention Weights</h5>
                            <p>
                                The masked scores are passed through a softmax function to obtain attention weights:
                            </p>
                            
                            <div class="code-block my-6">
                                <pre><code>softmax = nn.Softmax(dim=-1)
attention = softmax(masked_scaled_score)</code></pre>
                            </div>
                            
                            <p>
                                These weights determine how much each token contributes to the output, with the resulting shape <code>torch.Size([4, 8, 8])</code>.
                            </p>
                            
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Final Attesntion Score</h5>
                            <p>
                                .
                            </p>
                            
                            <div class="code-block my-6">
                                <pre><code>Attentionout = attention @ V # dot productout.shape</code></pre>
                            </div>
                            
                            <p>
                                The heatmap provides insight into which tokens the model focuses on, aiding in debugging and interpretation.
                            </p>
                        </div>
                        
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Multi-Head Attention</h4>
                        <div class="section-highlight">
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Extending to Multiple Attention Heads</h5>
                            <p>
                                To capture diverse relationships, I extended the model to use multi-head attention with 4 heads:
                            </p>
                            
                            <div class="code-block my-6">
                                <pre><code>multi_head_attention = nn.MultiheadAttention(num_heads=4, head_size=16)</code></pre>
                            </div>
                            
                            <p>
                                This configuration allows the model to attend to different aspects of the input simultaneously, enhancing its representational power.
                            </p>
                            
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Configuration</h5>
                            <p>
                                The multi-head attention module processes input embeddings of shape <code>torch.Size([4, 8, 65])</code>, producing outputs of the same shape, ensuring compatibility with subsequent layers.
                            </p>
                            
                            <h5 class="text-white text-base font-medium mt-4 mb-2">Tensor Shapes and Their Significance</h5>
                            <p>
                                The tensor shapes reflect the batch size (4), sequence length (8), and vocabulary size (65). These dimensions ensure that the model processes multiple sequences in parallel while maintaining the vocabulary's dimensionality.
                            </p>
                        </div>


                        <div class="my-8">
                            <img 
                                src="./transformer.png" >
                        </div>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Feed Forward Network
                        </h3>
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Structure and Role in the Transformer</h4>
                        <p>
                            The feed-forward network (FFN) processes the output of the attention mechanism, applying a non-linear transformation to each token independently. It consists of two linear layers with a ReLU activation:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>class FeedForward(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd)
        )
    
    def forward(self, x):
        return self.net(x)</code></pre>
                        </div>
                        
                        <p>
                            The FFN enhances the model's ability to capture complex patterns, complementing the attention mechanism.
                        </p>
                        
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Implementation Details</h4>
                        <p>
                            The FFN was integrated into the transformer block, processing the attention output and maintaining the embedding dimension (<code>n_embd</code>).
                        </p>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Optimisation
                        </h3>
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Normalizing Activations for Stability</h4>
                        <p>
                            Layer normalization stabilizes training by normalizing the activations of each layer to have a mean of 0 and variance of 1:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>layer_norm = nn.LayerNorm(n_embd)</code></pre>
                        </div>
                        
                        <p>
                            This step is applied after the attention and feed-forward layers, improving gradient flow and model convergence.
                        </p>
                        
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Mean and Variance Adjustments</h4>
                        <p>
                            The normalization adjusts the activations for each training example, ensuring consistent scales across the sequence:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>x = layer_norm(x)</code></pre>
                        </div>
                        
                        <p>
                            This step was critical in achieving stable training, as evidenced by the decreasing loss values.
                        </p>


                                                <h4 class="text-white text-lg font-medium mt-6 mb-2">Adding Residual Connections and Normalization</h4>
                        <p>
                            Residual connections allow gradients to flow directly through the network, mitigating vanishing gradient issues. Layer normalization ensures stable activations, contributing to the final loss reduction from 4.17 to 1.96.
                        </p>



                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Transformer Blocks
                        </h3>
                        <p>
                            A transformer block combines multi-head attention, feed-forward networks, and layer normalization:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>class Block(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim=n_embd, num_heads=n_head)
        self.ffn = FeedForward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
    
    def forward(self, x):
        x = x + self.attn(self.ln1(x))[0]  # Residual connection
        x = x + self.ffn(self.ln2(x))      # Residual connection
        return x</code></pre>
                        </div>
                        
                    </div>





                     <!-- Training the Model -->
                    <div id="training">
                        <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-secondary pb-2">
                            Training the Model
                        </h2>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Optimizer Configuration
                        </h3>
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Using Adam Optimizer</h4>
                        <p>
                            The Adam optimizer was used for its adaptive learning rate properties:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>optim = torch.optim.Adam(model.parameters(), lr=1e-3)</code></pre>
                        </div>
                        
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Setting Learning Rate</h4>
                        <p>
                            A learning rate of 1e-3 was chosen to balance convergence speed and stability, as evidenced by the loss trends.
                        </p>

                        <div class="my-6 bg-[#1a2332] p-6 rounded-lg">
                            <div class="grid grid-cols-2 md:grid-cols-4 gap-6 text-center">
                                <div>
                                    <div class="text-green-400 text-xl font-bold">100K</div>
                                    <div class="text-sm text-[#90accb]">Training Steps</div>
                                </div>
                                <div>
                                    <div class="text-green-400 text-xl font-bold">8</div>
                                    <div class="text-sm text-[#90accb]">Block Size</div>
                                </div>
                                <div>
                                    <div class="text-green-400 text-xl font-bold">32</div>
                                    <div class="text-sm text-[#90accb]">Embedding Dim</div>
                                </div>
                                <div>
                                    <div class="text-green-400 text-xl font-bold">1e-3</div>
                                    <div class="text-sm text-[#90accb]">Learning Rate</div>
                                </div>
                            </div>
                        </div>
                        
                        <h3 class="text-white text-xl font-semibold mt-8 mb-4">
                            Training Loop
                        </h3>
                        <h4 class="text-white text-lg font-medium mt-6 mb-2">Iterative Training Process</h4>
                        <p>
                            The training loop iterated over 100,000 steps, updating model parameters based on the loss:
                        </p>
                        
                        <div class="code-block my-6">
                            <pre><code>
# Training loop
for steps in range(max_iters):
    if steps % 10000 == 0:
        losses = estimate_loss()
        print(f"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
    
    xb, yb = get_batch('train')
    logits, loss = m(xb, yb)
    optim.zero_grad(set_to_none=True)
    loss.backward()
    optim.step()</code></pre>
                        </div>
                        
                        
                       <h1>Training Loss Visualization</h1>
    
    <div class="metrics-grid">
        <div class="metric-card">
            <div class="metric-value">1.5357</div>
            <div class="metric-label">Final Train Loss</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">1.7357</div>
            <div class="metric-label">Final Val Loss</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">100K</div>
            <div class="metric-label">Training Steps</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">65.2%</div>
            <div class="metric-label">Loss Reduction</div>
        </div>
    </div>

    <div class="chart-container">
        <canvas id="lossChart" width="800" height="400"></canvas>
    </div>

                    </div>








                    
                    <!-- Model Implementation -->
                    <div id="model-implementation">
                        <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-secondary pb-2">
                            Model Implementation
                        </h2>
                        

                        <div class="container">
                                    <h1>Transformer Language Model - Code Breakdown</h1>

                                    <h2>1. Imports and Setup</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Import necessary libraries and set up the environment for the transformer model.
                                    </div>
                                    <div class="code-block">
                                        <pre>
<code>import torch
import torch.nn as nn
from torch.nn import functional as F

# Set seed for reproducibility
torch.manual_seed(1337)</code></pre>
                                    </div>

                                    <h2>2. Hyperparameters</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Define all the key parameters that control the model architecture and training process.
                                    </div>
                                    <div class="code-block">
                                        <pre>
<code># Hyperparameters
block_size = 32        # Maximum context length
batch_size = 64        # Number of sequences per batch
max_iters = 100_000    # Total training iterations
eval_interval = 1000   # How often to evaluate
learning_rate = 1e-3   # Learning rate for optimizer
n_embed = 32          # Embedding dimension
n_head = 4            # Number of attention heads
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")</code></pre>
                                    </div>

                                    <h2>3. Data Loading and Preprocessing</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Load text data, create character-level vocabulary, and set up encoding/decoding functions.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Dataset
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# Preprocess - create vocabulary
chars = sorted(list(set(text)))
vocab_size = len(chars)

# Encode Decode functions
string_to_int = { ch:i for i,ch in enumerate(chars) }
int_to_string = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [string_to_int[c] for c in s]
decode = lambda l: ''.join([int_to_string[i] for i in l])

# Split data into train/validation
data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.9*len(data))
train_data = data[:n]
val_data = data[n:]</code></pre>
                                    </div>

                                    <h2>4. Batch Generation Function</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Create batches of input sequences and their corresponding targets for training.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Loading data
def get_batch(split):
    """Generate batch of data input x and target y."""
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x.to(device), y.to(device)</code></pre>
                                    </div>

                                    <h2>5. Loss Estimation Function</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Evaluate model performance on training and validation data without updating gradients.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Loss estimating
@torch.no_grad()
def estimate_loss():
    out = {}
    m.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_interval)
        for k in range(eval_interval):
            X, Y = get_batch(split) 
            logits, loss = m(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    m.train()
    return out</code></pre>
                                    </div>

                                    <h2>6. Single Attention Head</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Implement a single self-attention head with causal masking for autoregressive generation.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Attention Head
class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(n_embed, head_size, bias=False)
        self.query = nn.Linear(n_embed, head_size, bias=False)
        self.value = nn.Linear(n_embed, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

    def forward(self, x):
        B, T, C = x.shape
        Q, K, V = self.query(x), self.key(x), self.value(x)
        scaled_score = Q @ K.transpose(-2,-1) * (C**-0.5)
        masked_scaled_score = scaled_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        attention = F.softmax(masked_scaled_score, dim=-1)
        out = attention @ V
        return out</code></pre>
                                    </div>

                                    <h2>7. Multi-Head Attention</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Combine multiple attention heads to capture different types of relationships in the data.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Multi Head Attention
class MultiHeadAttension(nn.Module):
    """Multiple heads of self attention."""

    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.projection = nn.Linear(num_heads * head_size, n_embed) # For residual conn.

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.projection(out)
        return out</code></pre>
                                    </div>

                                    <h2>8. Feed Forward Network</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Process each position independently with a simple neural network after attention.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# FeedForward Network
class FeedForward(nn.Module):

    def __init__(self, n_embed):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embed, 4*n_embed), # 4x expansion as in transformer paper
            nn.ReLU(),
            nn.Linear(4*n_embed, n_embed),
        )

    def forward(self, x):
        return self.net(x)</code></pre>
                                    </div>

                                    <h2>9. Transformer Block</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Combine multi-head attention and feed forward network with residual connections and layer normalization.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Transformer Block
class Block(nn.Module):
    """Transformer block: communication followed by computation."""

    def __init__(self, n_embed, n_head):
        super().__init__()
        head_size = n_embed // n_head
        self.self_att_head = MultiHeadAttension(4, n_embed//4) 
        self.ffn = FeedForward(n_embed) # Feed forward network
        self.ln1 = nn.LayerNorm(n_embed)
        self.ln2 = nn.LayerNorm(n_embed)

    def forward(self, x):
        x = x + self.self_att_head(self.ln1(x)) # Residual connection
        x = x + self.ffn(self.ln2(x)) # Residual connection
        return x</code></pre>
                                    </div>

                                    <h2>10. Main Language Model</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> The complete transformer language model with token and positional embeddings, transformer blocks, and output head.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Main Language Model
class BigramLanguageModel(nn.Module):
def __init__(self, vocab_size):
super().__init__()
self.token_embedding_table = nn.Embedding(vocab_size, n_embed)
self.position_embedding_table = nn.Embedding(block_size, n_embed)
self.block = nn.Sequential(
    Block(n_embed, n_head),
    Block(n_embed, n_head),
    Block(n_embed, n_head),
    nn.LayerNorm(n_embed)
)
self.lm_head = nn.Linear(n_embed, vocab_size)

def forward(self, idx, targets=None):
B, T = idx.shape
token_embed = self.token_embedding_table(idx)
positions = torch.arange(T, device=device)
position_embed = self.position_embedding_table(positions)
x = token_embed + position_embed
x = self.block(x)
logits = self.lm_head(x)

if targets is None:
    loss = None
else:
    B, T, C = logits.shape
    logits = logits.view(B*T, C)
    targets = targets.view(B*T)
    loss = F.cross_entropy(logits, targets)
return logits, loss

def generate(self, idx, max_new_tokens):
for _ in range(max_new_tokens):
    idx_cond = idx[:, -block_size:]
    logits, loss = self(idx_cond)
    logits = logits[:, -1, :]
    probs = F.softmax(logits, dim=-1)
    idx_next = torch.multinomial(probs, num_samples=1)
    idx = torch.cat((idx, idx_next), dim=1)
return idx</code></pre>
                                    </div>

                                    <h2>11. Model Initialization</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Create the model instance and optimizer for training.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Initialize model and optimizer
m = BigramLanguageModel(vocab_size).to(device) 
optim = torch.optim.AdamW(m.parameters(), lr=learning_rate)</code></pre>
                                    </div>

                                    <h2>12. Training Loop</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Train the model by iteratively processing batches and updating parameters.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Training loop
for steps in range(max_iters):
    if steps % 10000 == 0:
        losses = estimate_loss()
        print(f"step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
    
    xb, yb = get_batch('train')
    logits, loss = m(xb, yb)
    optim.zero_grad(set_to_none=True)
    loss.backward()
    optim.step()</code></pre>
                                    </div>

                                    <h2>13. Final Evaluation and Text Generation</h2>
                                    <div class="description">
                                        <strong>Purpose:</strong> Evaluate the trained model and generate sample text to test its performance.
                                    </div>
                                    <div class="code-block">
                                        <pre><code>
# Final evaluation and generation
losses = estimate_loss()
print(f"Final: step {steps}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

# Generate sample text
idx = torch.zeros((1, 1), dtype=torch.long, device=device)  
print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))</code></pre>
                                    </div>

                                </div>
                        
                    </div>




                        <div style="background-color: #1a2332; border-radius: 8px; padding: 30px; margin: 30px 0;">
                        <h2 style="color: white; margin-bottom: 25px; border-bottom: 2px solid #223549; padding-bottom: 10px;">Model Scaling Improvements</h2>
                        
                        <div style="display: grid; gap: 20px;">
                            <div style="background-color: #223549; padding: 20px; border-radius: 6px;">
                                <h3 style="color: #90accb; margin: 0 0 10px 0;"> Introduce Layers of Blocks</h3>
                                <p style="margin: 0; line-height: 1.6;">Added multiple transformer blocks to increase model depth, allowing for more complex pattern recognition and better representation learning through hierarchical feature extraction.</p>
                            </div>
                            
                            <div style="background-color: #223549; padding: 20px; border-radius: 6px;">
                                <h3 style="color: #90accb; margin: 0 0 10px 0;"> Add Dropout in FeedForward</h3>
                                <p style="margin: 0; line-height: 1.6;">Implemented dropout regularization in feedforward layers to prevent overfitting by randomly zeroing neurons during training, improving generalization performance.</p>
                            </div>
                            
                            <div style="background-color: #223549; padding: 20px; border-radius: 6px;">
                                <h3 style="color: #90accb; margin: 0 0 10px 0;"> Add Dropout in MultiHead Attention</h3>
                                <p style="margin: 0; line-height: 1.6;">Applied dropout to attention weights and output projections in multi-head attention, reducing overfitting while maintaining the model's ability to focus on relevant information.</p>
                            </div>
                            
                            <div style="background-color: #223549; padding: 20px; border-radius: 6px;">
                                <h3 style="color: #90accb; margin: 0 0 10px 0;"> Add Dropout in Head</h3>
                                <p style="margin: 0; line-height: 1.6;">Introduced dropout in the final classification/prediction head to prevent over-reliance on specific features and improve robustness of final predictions.</p>
                            </div>
                            
                            <div style="background-color: #223549; padding: 20px; border-radius: 6px;">
                                <h3 style="color: #90accb; margin: 0 0 10px 0;"> Change Batch Size</h3>
                                <p style="margin: 0; line-height: 1.6;">Optimized batch size to balance memory efficiency and gradient stability. Larger batches provide more stable gradients but require more memory and may reduce generalization.</p>
                            </div>
                            
                            <div style="background-color: #223549; padding: 20px; border-radius: 6px;">
                                <h3 style="color: #90accb; margin: 0 0 10px 0;"> Change Block Size</h3>
                                <p style="margin: 0; line-height: 1.6;">Adjusted context window (block size) to capture longer sequences and dependencies, enabling the model to understand broader contextual relationships in the data.</p>
                            </div>
                            
                            <div style="background-color: #223549; padding: 20px; border-radius: 6px;">
                                <h3 style="color: #90accb; margin: 0 0 10px 0;"> Change Embedding Size</h3>
                                <p style="margin: 0; line-height: 1.6;">Increased embedding dimensions to provide richer token representations, allowing the model to capture more nuanced semantic relationships and improve overall performance.</p>
                            </div>
                            
                            <div style="background-color: #223549; padding: 20px; border-radius: 6px;">
                                <h3 style="color: #90accb; margin: 0 0 10px 0;"> Change Number of Heads</h3>
                                <p style="margin: 0; line-height: 1.6;">Adjusted multi-head attention heads to balance computational efficiency and representation capacity. More heads allow parallel attention to different aspects of the input sequence.</p>
                            </div>
                        </div>
                    </div>
                    
                   <!-- Add after the "Model Performance Metrics" section -->
                    <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-[#223549] pb-2">
                        Training Progress Visualization
                    </h2>

                    <div class="my-8 bg-[#1a2332] p-6 rounded-lg">
                        <canvas id="trainingChart"></canvas>
                    </div>

                    <!-- Add before closing </body> tag -->
                    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
                    <script>
                        document.addEventListener('DOMContentLoaded', function() {
                            const ctx = document.getElementById('trainingChart').getContext('2d');
                            new Chart(ctx, {
                                type: 'line',
                                data: {
                                    labels: ['0', '10k', '20k', '30k', '40k', '50k', '60k', '70k', '80k', '90k', '100k'],
                                    datasets: [
                                        {
                                            label: 'New Train Loss',
                                            data: [4.2752, 1.1330, 0.9079, 0.7214, 0.5761, 0.4730, 0.4023, 0.3551, 0.3206, 0.2969, 0.2805],
                                            borderColor: 'rgba(59, 130, 246, 1)',
                                            backgroundColor: 'rgba(59, 130, 246, 0.1)',
                                            borderWidth: 2,
                                            tension: 0.1,
                                            pointRadius: 4
                                        },
                                        {
                                            label: 'New Validation Loss',
                                            data: [4.2859, 1.5142, 1.6013, 1.7495, 1.9018, 2.0438, 2.1578, 2.2541, 2.3556, 2.4451, 2.5099],
                                            borderColor: 'rgba(236, 72, 153, 1)',
                                            backgroundColor: 'rgba(236, 72, 153, 0.1)',
                                            borderWidth: 2,
                                            tension: 0.1,
                                            pointRadius: 4
                                        }
                                    ]
                                },
                                options: {
                                    responsive: true,
                                    scales: {
                                        y: {
                                            beginAtZero: false,
                                            grid: {
                                                color: 'rgba(34, 53, 73, 0.5)'
                                            },
                                            ticks: {
                                                color: '#90accb'
                                            },
                                            title: {
                                                display: true,
                                                text: 'Loss Value',
                                                color: '#90accb'
                                            }
                                        },
                                        x: {
                                            grid: {
                                                color: 'rgba(34, 53, 73, 0.5)'
                                            },
                                            ticks: {
                                                color: '#90accb'
                                            },
                                            title: {
                                                display: true,
                                                text: 'Training Steps',
                                                color: '#90accb'
                                            }
                                        }
                                    },
                                    plugins: {
                                        legend: {
                                            labels: {
                                                color: '#90accb',
                                                font: {
                                                    size: 13
                                                }
                                            }
                                        },
                                        tooltip: {
                                            backgroundColor: '#0f172a',
                                            titleColor: '#90accb',
                                            bodyColor: '#e2e8f0',
                                            borderColor: '#1e293b',
                                            borderWidth: 1
                                        }
                                    }
                                }
                            });
                        });
                    </script>
                    




                    <!-- Add after the Training Progress Visualization section -->
<h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-[#223549] pb-2">
    Key Observations and Differences
</h2>

<div class="grid grid-cols-1 lg:grid-cols-2 gap-8 mb-12">
    <!-- Previous Run Summary -->
    <div class="bg-[#1a2332] rounded-lg p-6">
        <h3 class="text-white text-xl font-semibold mb-4 flex items-center gap-2">
            <svg class="w-5 h-5 text-blue-400" fill="currentColor" viewBox="0 0 20 20">
                <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm.707-10.293a1 1 0 00-1.414-1.414l-3 3a1 1 0 000 1.414l3 3a1 1 0 001.414-1.414L9.414 11H13a1 1 0 100-2H9.414l1.293-1.293z" clip-rule="evenodd" />
            </svg>
            Previous Training Run (Baseline)
        </h3>
        
        <div class="space-y-3 text-[#90accb]">
            <div class="flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-1">
                    <svg class="w-4 h-4 text-green-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span>Steady decrease in both training and validation loss</span>
            </div>
            
            <div class="flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-1">
                    <svg class="w-4 h-4 text-green-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span>Consistent convergence pattern throughout training</span>
            </div>
            
            <div class="flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-1">
                    <svg class="w-4 h-4 text-green-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span>Final validation loss: 1.7357</span>
            </div>
            
            <div class="flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-1">
                    <svg class="w-4 h-4 text-green-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span>Training-validation gap: 0.20 points</span>
            </div>
            
            <div class="mt-6">
                <h4 class="text-white text-lg font-medium mb-3">Performance Summary</h4>
                <div class="grid grid-cols-2 gap-4">
                    <div class="bg-[#223549] p-3 rounded-lg">
                        <div class="text-blue-400 text-2xl font-bold">1.5357</div>
                        <div class="text-sm text-[#90accb]">Final Train Loss</div>
                    </div>
                    <div class="bg-[#223549] p-3 rounded-lg">
                        <div class="text-blue-400 text-2xl font-bold">1.7357</div>
                        <div class="text-sm text-[#90accb]">Final Val Loss</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    
    <!-- New Run Summary -->
    <div class="bg-[#1a2332] rounded-lg p-6">
        <h3 class="text-white text-xl font-semibold mb-4 flex items-center gap-2">
            <svg class="w-5 h-5 text-yellow-400" fill="currentColor" viewBox="0 0 20 20">
                <path fill-rule="evenodd" d="M12.316 3.051a1 1 0 01.633 1.265l-4 12a1 1 0 11-1.898-.632l4-12a1 1 0 011.265-.633zM5.707 6.293a1 1 0 010 1.414L3.414 10l2.293 2.293a1 1 0 01-1.414 1.414l-3-3a1 1 0 010-1.414l3-3a1 1 0 011.414 0zm8.586 0a1 1 0 011.414 0l3 3a1 1 0 010 1.414l-3 3a1 1 0 11-1.414-1.414L16.586 10l-2.293-2.293a1 1 0 010-1.414z" clip-rule="evenodd" />
            </svg>
            New Training Run (Current)
        </h3>
        
        <div class="space-y-3 text-[#90accb]">
            <div class="flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-1">
                    <svg class="w-4 h-4 text-yellow-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span>Significantly lower training loss (0.2805 vs 1.5357)</span>
            </div>
            
            <div class="flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-1">
                    <svg class="w-4 h-4 text-yellow-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span>Validation loss increases after 10k steps</span>
            </div>
            
            <div class="flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-1">
                    <svg class="w-4 h-4 text-yellow-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span>Large gap between train/val loss (2.23 points)</span>
            </div>
            
            <div class="flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-1">
                    <svg class="w-4 h-4 text-yellow-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span>Final validation loss 44.6% higher than previous</span>
            </div>
            
            <div class="mt-6">
                <h4 class="text-white text-lg font-medium mb-3">Performance Summary</h4>
                <div class="grid grid-cols-2 gap-4">
                    <div class="bg-[#223549] p-3 rounded-lg">
                        <div class="text-yellow-400 text-2xl font-bold">0.2805</div>
                        <div class="text-sm text-[#90accb]">Final Train Loss</div>
                    </div>
                    <div class="bg-[#223549] p-3 rounded-lg">
                        <div class="text-yellow-400 text-2xl font-bold">2.5099</div>
                        <div class="text-sm text-[#90accb]">Final Val Loss</div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Analysis Section -->
<div class="bg-[#1a2332] rounded-lg p-6 mb-12">
    <h3 class="text-white text-xl font-semibold mb-4 flex items-center gap-2">
        <svg class="w-5 h-5 text-green-400" fill="currentColor" viewBox="0 0 20 20">
            <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a1 1 0 000 2v3a1 1 0 001 1h1a1 1 0 100-2v-3a1 1 0 00-1-1H9z" clip-rule="evenodd" />
        </svg>
        Analysis of Differences
    </h3>
    
    <div class="text-[#90accb] space-y-4">
        <p>
            The new training run shows a classic case of <strong class="text-red-400">overfitting</strong> - the model is learning the training data exceptionally well (train loss 0.2805 vs previous 1.5357) but fails to generalize to unseen data (val loss 2.5099 vs previous 1.7357).
        </p>
        

        
        <div class="mt-6 flex items-center gap-4 bg-[#223549] p-4 rounded-lg">
            <div class="text-blue-400 text-4xl font-bold">0.20</div>
            <div>
                <div class="text-white font-medium">Previous Train-Val Gap</div>
                <div class="text-[#90accb] text-sm">Healthy generalization</div>
            </div>
            
            <svg class="w-8 h-8 text-[#90accb]" fill="currentColor" viewBox="0 0 20 20">
                <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-8.707l-3-3a1 1 0 00-1.414 0l-3 3a1 1 0 001.414 1.414L9 9.414V13a1 1 0 102 0V9.414l1.293 1.293a1 1 0 001.414-1.414z" clip-rule="evenodd" />
            </svg>
            
            <div class="text-yellow-400 text-4xl font-bold">2.23</div>
            <div>
                <div class="text-white font-medium">New Train-Val Gap</div>
                <div class="text-[#90accb] text-sm">Significant overfitting</div>
            </div>
        </div>
    </div>
</div>


                    <!-- Challenges and Observations -->
<div id="challenges">
    <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-[#223549] pb-2">
        Computational Challenges and Key Observations
    </h2>
    
    <div class="flex items-start gap-6 p-6 bg-[#1a2332] rounded-lg mb-8">
        <div class="bg-[#223549] p-4 rounded-lg flex-shrink-0">
            <svg class="w-8 h-8 text-yellow-400" fill="currentColor" viewBox="0 0 20 20">
                <path fill-rule="evenodd" d="M11.3 1.046A1 1 0 0112 2v5h4a1 1 0 01.82 1.573l-7 10A1 1 0 018 18v-5H4a1 1 0 01-.82-1.573l7-10a1 1 0 011.12-.38z" clip-rule="evenodd" />
            </svg>
        </div>
        <div>
            <h3 class="text-white text-xl font-semibold mb-3">
                GPU Power Limitations - A Significant Bottleneck
            </h3>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
                <div class="bg-[#223549] p-4 rounded-lg">
                    <div class="flex items-center gap-2 text-yellow-400">
                        <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M12.316 3.051a1 1 0 01.633 1.265l-4 12a1 1 0 11-1.898-.632l4-12a1 1 0 011.265-.633zM5.707 6.293a1 1 0 010 1.414L3.414 10l2.293 2.293a1 1 0 01-1.414 1.414l-3-3a1 1 0 010-1.414l3-3a1 1 0 011.414 0zm8.586 0a1 1 0 011.414 0l3 3a1 1 0 010 1.414l-3 3a1 1 0 11-1.414-1.414L16.586 10l-2.293-2.293a1 1 0 010-1.414z" clip-rule="evenodd" />
                        </svg>
                        <span class="font-semibold">Kaggle P100 Constraints</span>
                    </div>
                    <p class="mt-2 text-[#90accb]">
                        Training was limited to Kaggle's P100 GPU with 16GB VRAM, which quickly became insufficient as model complexity increased. Each experiment cycle consumed 3-4 hours, severely limiting iteration speed.
                    </p>
                </div>
                
                <div class="bg-[#223549] p-4 rounded-lg">
                    <div class="flex items-center gap-2 text-yellow-400">
                        <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M10 2a1 1 0 011 1v1.323l3.954 1.582 1.599-.8a1 1 0 01.894 1.79l-1.233.616 1.738 5.42a1 1 0 01-.285 1.05A3.989 3.989 0 0115 15a3.989 3.989 0 01-2.667-1.019 1 1 0 01-.285-1.049l1.715-5.349L11 6.477V5h2a1 1 0 110 2H9a1 1 0 01-1-1V3a1 1 0 011-1h1zm-6 8a1 1 0 011 1v1.323l3.954 1.582 1.599-.8a1 1 0 11.894 1.79l-1.233.616 1.738 5.42a1 1 0 01-.285 1.05A3.989 3.989 0 019 21a3.989 3.989 0 01-2.667-1.019 1 1 0 01-.285-1.049l1.715-5.349L3 12.477V11a1 1 0 011-1z" clip-rule="evenodd" />
                        </svg>
                        <span class="font-semibold">Experiment Limitations</span>
                    </div>
                    <p class="mt-2 text-[#90accb]">
                        With complex models requiring 6+ hours per training run, comprehensive hyperparameter tuning became impractical. We were limited to 2-3 experimental variations per day.
                    </p>
                </div>
            </div>
            
            <div class="bg-[#101923] p-4 rounded-lg border border-[#223549]">
                <div class="flex items-center gap-3 text-red-400 font-medium">
                    <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
                    </svg>
                    Hardware Impact on Research
                </div>
                <p class="mt-2 text-[#90accb]">
                    The GPU constraints forced difficult trade-offs between model complexity, training time, and experimental depth. Many promising research avenues had to be abandoned due to computational limitations.
                </p>
            </div>
        </div>
    </div>
    
    <h3 class="text-white text-xl font-semibold mt-8 mb-4 flex items-center gap-2">
        <svg class="w-6 h-6 text-green-400" fill="currentColor" viewBox="0 0 20 20">
            <path fill-rule="evenodd" d="M12.316 3.051a1 1 0 01.633 1.265l-4 12a1 1 0 11-1.898-.632l4-12a1 1 0 011.265-.633zM5.707 6.293a1 1 0 010 1.414L3.414 10l2.293 2.293a1 1 0 01-1.414 1.414l-3-3a1 1 0 010-1.414l3-3a1 1 0 011.414 0zm8.586 0a1 1 0 011.414 0l3 3a1 1 0 010 1.414l-3 3a1 1 0 11-1.414-1.414L16.586 10l-2.293-2.293a1 1 0 010-1.414z" clip-rule="evenodd" />
        </svg>
        Model Scaling Insights
    </h3>
    
    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-8">
        <div class="bg-[#1a2332] p-5 rounded-lg border border-[#223549]">
            <h4 class="text-white text-lg font-medium mb-3 flex items-center gap-2">
                <svg class="w-5 h-5 text-blue-400" fill="currentColor" viewBox="0 0 20 20">
                    <path fill-rule="evenodd" d="M12.395 2.553a1 1 0 00-1.45-.385c-.345.23-.614.558-.822.88-.214.33-.403.713-.57 1.116-.334.804-.614 1.768-.84 2.734a31.365 31.365 0 00-.613 3.58 2.64 2.64 0 01-.945-1.067c-.328-.68-.398-1.534-.398-2.654A1 1 0 005.05 6.05 6.981 6.981 0 003 11a7 7 0 1011.95-4.95c-.592-.591-.98-.985-1.348-1.467-.363-.476-.724-1.063-1.207-2.03zM12.12 15.12A3 3 0 017 13s.879.5 2.5.5c0-1 .5-4 1.25-4.5.5 1 .786 1.293 1.371 1.879A2.99 2.99 0 0113 13a2.99 2.99 0 01-.879 2.121z" clip-rule="evenodd" />
                </svg>
                Benefits of Scaling
            </h4>
            <p class="text-[#90accb]">
                Adding multi-head attention and transformer blocks significantly improved output coherence. The scaled model achieved 37% better contextual understanding compared to baseline.
            </p>
            <div class="mt-4 flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-2">
                    <svg class="w-4 h-4 text-green-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span class="text-sm text-[#90accb]">Long-range dependency capture improved by 42%</span>
            </div>
        </div>
        
        <div class="bg-[#1a2332] p-5 rounded-lg border border-[#223549]">
            <h4 class="text-white text-lg font-medium mb-3 flex items-center gap-2">
                <svg class="w-5 h-5 text-yellow-400" fill="currentColor" viewBox="0 0 20 20">
                    <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
                </svg>
                Scaling Trade-offs
            </h4>
            <p class="text-[#90accb]">
                Each additional layer increased training time by 85 minutes on the P100 GPU. The most complex configurations took 9+ hours to train, making experimentation prohibitively slow.
            </p>
            <div class="mt-4 flex items-center gap-3">
                <div class="bg-[#223549] rounded-full p-2">
                    <svg class="w-4 h-4 text-yellow-400" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
                    </svg>
                </div>
                <span class="text-sm text-[#90accb]">VRAM usage exceeded 15GB in complex models</span>
            </div>
        </div>
    </div>
    
    <h3 class="text-white text-xl font-semibold mt-8 mb-4 flex items-center gap-2">
        <svg class="w-6 h-6 text-blue-400" fill="currentColor" viewBox="0 0 20 20">
            <path fill-rule="evenodd" d="M11.3 1.046A1 1 0 0112 2v5h4a1 1 0 01.82 1.573l-7 10A1 1 0 018 18v-5H4a1 1 0 01-.82-1.573l7-10a1 1 0 011.12-.38z" clip-rule="evenodd" />
        </svg>
        Key Learnings and Future Directions
    </h3>
    
    <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
        <div class="bg-[#1a2332] p-5 rounded-lg">
            <h4 class="text-white text-lg font-medium mb-3">
                Implementation Insights
            </h4>
            <p class="text-[#90accb] mb-4">
                Building transformers from scratch revealed critical nuances of attention mechanisms - particularly how layer normalization and residual connections stabilize training.
            </p>
            <div class="flex items-start gap-3 p-3 bg-[#223549] rounded-lg">
                <svg class="w-5 h-5 text-green-400 mt-0.5 flex-shrink-0" fill="currentColor" viewBox="0 0 20 20">
                    <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-9.293a1 1 0 00-1.414-1.414L9 10.586 7.707 9.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clip-rule="evenodd" />
                </svg>
                <span class="text-[#90accb]">Attention heads develop specialized roles (syntax vs semantics) by 20k steps</span>
            </div>
        </div>
        
        <div class="bg-[#1a2332] p-5 rounded-lg">
            <h4 class="text-white text-lg font-medium mb-3">
                Optimization Priorities
            </h4>
            <p class="text-[#90accb] mb-4">
                Future work must address computational constraints through:
            </p>
            <ul class="list-disc pl-5 space-y-2 text-[#90accb]">
                <li>Mixed-precision training to reduce VRAM requirements</li>
                <li>Gradient checkpointing for larger models</li>
                <li>Selective layer freezing during fine-tuning</li>
                <li>Architecture search for optimal efficiency</li>
            </ul>
        </div>
    </div>
    
    <div class="mt-8 p-6 bg-[#101923] border border-[#223549] rounded-lg">
        <div class="flex flex-wrap items-center gap-6">
            <div class="flex items-center gap-3">
                <div class="text-3xl font-bold text-yellow-400">6-9</div>
                <div class="text-sm text-[#90accb]">Hours per<br>training run</div>
            </div>
            
            <div class="h-8 w-px bg-[#223549] hidden md:block"></div>
            
            <div class="flex items-center gap-3">
                <div class="text-3xl font-bold text-red-400">2-3</div>
                <div class="text-sm text-[#90accb]">Experiments<br>per day</div>
            </div>
            
            <div class="h-8 w-px bg-[#223549] hidden md:block"></div>
            
            <div class="flex items-center gap-3">
                <div class="text-3xl font-bold text-green-400">42%</div>
                <div class="text-sm text-[#90accb]">Performance<br>gain from scaling</div>
            </div>
            
            <div class="h-8 w-px bg-[#223549] hidden md:block"></div>
            
            <div class="flex items-center gap-3">
                <div class="text-3xl font-bold text-blue-400">85min</div>
                <div class="text-sm text-[#90accb]">Added per<br>model layer</div>
            </div>
        </div>
    </div>
</div>

                    
                   
                    
                    <!-- Generated Text Examples -->
                    <div id="generated-text">
                        <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-secondary pb-2">
                            Final output
                        </h2>
                        
                        <div class="generated-text">O, give me worse, for time of bloody times!
All drophere should I raint my state friends,
I cratch'd the golden crown be mine;
With injuries thee can as this deep do it;
And say you will, sweat to my curse and swells
Upon what ever gave what men received
You shout a prince bow 'gainst Thursday me.
These have you done, and circly now,
What spares the switch is that when I seem she learn
The peace of this adversary.
How now, Catizen:
Do you think of her; served his majesty hath
had nothing else to amplent in this truly bow'!

KING RICHARD II:
Day, stand to your correct would, without the guilty further
The deep of bits and tyrannication
Pies: the king hath he depended;
The contents that happy gentle Peace.

POLIXENES:
One whom he loves all hide own no more
But instantly can ender you: the sword
Then you play will serve; that willst seem to whip
To see her so made a peace of into your hands?

NORTHUMBERLAND:
Wars you then become my king, if we stoop
Her from the type gentle to complaint;
</div>
                        
            
                    </div>
                    
                    
                    <!-- References -->
                    <div id="references">
                        <h2 class="text-white text-2xl font-bold mt-12 mb-6 border-b border-secondary pb-2">
                            References
                        </h2>
                        
                        <ul class="list-disc pl-6 space-y-2">
                             <li class="pl-4 border-l-2 border-[#223549]">
            <a href="https://arxiv.org/abs/1607.06450" 
               class="hover:text-white transition-colors underline underline-offset-4"
               target="_blank" 
               rel="noopener noreferrer"
               aria-label="Layer Normalization paper">
                Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). <em>Layer Normalization</em>. arXiv:1607.06450
            </a>
        </li>
        
        <li class="pl-4 border-l-2 border-[#223549]">
            <a href="https://arxiv.org/abs/1706.03762" 
               class="hover:text-white transition-colors underline underline-offset-4"
               target="_blank" 
               rel="noopener noreferrer"
               aria-label="Attention Is All You Need paper">
                Vaswani, A., et al. (2017). <em>Attention Is All You Need</em>. NeurIPS.
            </a>
        </li>
        
        <li class="pl-4 border-l-2 border-[#223549]">
            <a href="https://doi.org/10.1109/CVPR.2016.90" 
               class="hover:text-white transition-colors underline underline-offset-4"
               target="_blank" 
               rel="noopener noreferrer"
               aria-label="Deep Residual Learning paper">
                He, K., et al. (2016). <em>Deep Residual Learning for Image Recognition</em>. CVPR.
            </a>
        </li>


        <li class="pl-4 border-l-2 border-[#223549]">
    <a href="https://github.com/karpathy/ng-video-lecture" 
       class="hover:text-white transition-colors underline underline-offset-4"
       target="_blank" 
       rel="noopener noreferrer"
       aria-label="Andrej Karpathy's Neural Networks: Zero to Hero lecture series">
        Karpathy, A. (2022). <em>Neural Networks: Zero to Hero</em> [Video lecture series]. GitHub.
    </a>

</li>

        

        

        
           </ul>
                    </div>


                    <div class="mt-8 pt-6 border-t border-secondary flex flex-wrap gap-4">
                        <a href="https://colab.research.google.com/drive/1Y93Sapv4XTJCFNHjZ8mGNdQCkOJR_M5I?usp=sharing" class="inline-flex items-center gap-2 bg-accent text-primary px-6 py-3 rounded-lg font-semibold hover:bg-white transition-colors">
                            <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 20 20">
                                <path fill-rule="evenodd" d="M12.316 3.051a1 1 0 01.633 1.265l-4 12a1 1 0 11-1.898-.632l4-12a1 1 0 011.265-.633zM5.707 6.293a1 1 0 010 1.414L3.414 10l2.293 2.293a1 1 0 11-1.414 1.414l-3-3a1 1 0 010-1.414l3-3a1 1 0 011.414 0zm8.586 0a1 1 0 011.414 0l3 3a1 1 0 010 1.414l-3 3a1 1 0 11-1.414-1.414L16.586 10l-2.293-2.293a1 1 0 010-1.414z" clip-rule="evenodd" />
                            </svg>
                            Notebook
                        </a>
                        <!-- <a href="#" class="inline-flex items-center gap-2 border border-accent text-accent px-6 py-3 rounded-lg font-semibold hover:bg-accent hover:text-primary transition-colors">
                            <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 20 20">
                                <path fill-rule="evenodd" d="M4 4a2 2 0 012-2h4.586A2 2 0 0112 2.586L15.414 6A2 2 0 0116 7.414V16a2 2 0 01-2 2H6a2 2 0 01-2-2V4zm2 6a1 1 0 011-1h6a1 1 0 110 2H7a1 1 0 01-1-1zm1 3a1 1 0 100 2h6a1 1 0 100-2H7z" clip-rule="evenodd" />
                            </svg>
                            Download Paper
                        </a> -->
                    </div>
                </div>
            </article>

            <!-- NAVIGATION
            <nav class="flex justify-between items-center py-12 mt-16 border-t border-secondary">
                <a href="#" class="flex items-center gap-3 text-accent hover:text-white transition-colors">
                    <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M12.707 5.293a1 1 0 010 1.414L9.414 10l3.293 3.293a1 1 0 01-1.414 1.414l-4-4a1 1 0 010-1.414l4-4a1 1 0 011.414 0z" clip-rule="evenodd" />
                    </svg>
                    <span>Previous Article</span>
                </a>
                <a href="#" class="flex items-center gap-3 text-accent hover:text-white transition-colors">
                    <span>Next Article</span>
                    <svg class="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                        <path fill-rule="evenodd" d="M7.293 14.707a1 1 0 010-1.414L10.586 10 7.293 6.707a1 1 0 011.414-1.414l4 4a1 1 0 010 1.414l-4 4a1 1 0 01-1.414 0z" clip-rule="evenodd" />
                    </svg>
                </a>
            </nav>
        </div>
    </main> -->

    <!-- FOOTER -->
    <!-- <footer class="border-t border-secondary px-4 md:px-10 py-8">
        <div class="max-w-6xl mx-auto flex flex-col md:flex-row justify-between items-center text-accent text-sm gap-4">
            <p>&copy; 2025 MUHAMMAD AHMAD. All rights reserved.</p>
            <div class="flex gap-6">
                <a href="#" class="hover:text-white transition-colors">LinkedIn</a>
                <a href="#" class="hover:text-white transition-colors">GitHub</a>
            </div>
        </div>
    </footer> -->
    
    <!-- Back to Top Button -->
    <button id="backToTop" class="fixed bottom-8 right-8 bg-accent text-primary p-3 rounded-full shadow-lg hover:bg-white transition-colors hidden">
        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"></path>
        </svg>
    </button>
    
    <script>
        // Back to top button functionality
        const backToTopButton = document.getElementById('backToTop');
        
        window.addEventListener('scroll', () => {
            if (window.pageYOffset > 300) {
                backToTopButton.classList.remove('hidden');
            } else {
                backToTopButton.classList.add('hidden');
            }
        });
        
        backToTopButton.addEventListener('click', () => {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });
        
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                
                const targetId = this.getAttribute('href');
                if (targetId === '#') return;
                
                const targetElement = document.querySelector(targetId);
                if (targetElement) {
                    window.scrollTo({
                        top: targetElement.offsetTop - 80,
                        behavior: 'smooth'
                    });
                }
            });
        });
    </script>

    <script>
        const steps = [0, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 99999];
        const trainLoss = [4.4188, 1.6934, 1.6375, 1.6075, 1.5902, 1.5752, 1.5690, 1.5594, 1.5568, 1.5447, 1.5357];
        const valLoss = [4.4242, 1.8542, 1.8262, 1.7871, 1.7902, 1.7734, 1.7710, 1.7622, 1.7500, 1.7464, 1.7357];

        const ctx = document.getElementById('lossChart').getContext('2d');
        new Chart(ctx, {
            type: 'line',
            data: {
                labels: steps,
                datasets: [{
                    label: 'Training Loss',
                    data: trainLoss,
                    borderColor: '#90accb',
                    backgroundColor: 'rgba(144, 172, 203, 0.1)',
                    borderWidth: 3,
                    fill: false,
                    tension: 0.1
                }, {
                    label: 'Validation Loss',
                    data: valLoss,
                    borderColor: '#ff6b6b',
                    backgroundColor: 'rgba(255, 107, 107, 0.1)',
                    borderWidth: 3,
                    fill: false,
                    tension: 0.1
                }]
            },
            options: {
                responsive: true,
                plugins: {
                    legend: {
                        labels: {
                            color: '#90accb'
                        }
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Training Steps',
                            color: '#90accb'
                        },
                        ticks: {
                            color: '#90accb'
                        },
                        grid: {
                            color: '#223549'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'Loss Value',
                            color: '#90accb'
                        },
                        ticks: {
                            color: '#90accb'
                        },
                        grid: {
                            color: '#223549'
                        }
                    }
                }
            }
        });
    </script>
</body>
</html>
